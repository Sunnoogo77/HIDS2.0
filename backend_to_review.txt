---------- START OF FILE: backend/app/api/backend_logs.py ----------

from fastapi import APIRouter, Depends, HTTPException, Query
from app.core.security import get_current_active_user
from pathlib import Path
import os

router = APIRouter(
    prefix="/api/logs/backend",
    tags=["backend-logs"],
    dependencies=[Depends(get_current_active_user)]
)

LOG_DIR = Path(os.getenv("HIDS_LOG_DIR", "logs")).resolve()

def _safe_join(filename: str) -> Path:
    fp = (LOG_DIR / filename).resolve()
    if not str(fp).startswith(str(LOG_DIR)):
        raise HTTPException(status_code=400, detail="Invalid path")
    return fp

@router.get("/files")
def list_files():
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    files = [f.name for f in LOG_DIR.iterdir() if f.is_file() and f.name.lower().endswith((".log", ".log.txt", ".txt"))]
    files.sort()
    return {"files": files or ["app.log"]}

@router.get("")
def read_logs(
    file: str = Query("app.log"),
    page: int = Query(1, ge=1),
    limit: int = Query(15, ge=1, le=500),
    level: str | None = Query(None, description="DEBUG|INFO|WARNING|ERROR|CRITICAL"),
    contains: str | None = Query(None),
):
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    fp = _safe_join(file)
    if not fp.exists():
        return {"file": file, "page": 1, "page_count": 1, "lines": []}

    lines = fp.read_text(encoding="utf-8", errors="ignore").splitlines()

    def parse(line: str):
        l = line.strip()
        lv = None
        # Naïf : détecte un niveau de log standard
        for cand in ["DEBUG","INFO","WARNING","ERROR","CRITICAL"]:
            if f" {cand} " in l or l.startswith(cand) or f"[{cand}]" in l:
                lv = cand
                break
        # L'horodatage n'est pas garanti, on retourne la ligne brute pour la lisibilité
        return {"ts": "", "level": lv, "msg": l, "text": l}

    rows = [parse(l) for l in lines]

    if level:
        rows = [r for r in rows if (r["level"] or "").upper() == level.upper()]
    if contains:
        s = contains.lower()
        rows = [r for r in rows if s in (r["msg"] or "").lower()]

    total = len(rows)
    page_count = max(1, (total + limit - 1) // limit)
    page = min(page, page_count)
    start = (page - 1) * limit
    end = start + limit
    return {"file": file, "page": page, "page_count": page_count, "lines": rows[start:end]}

---------- END OF FILE: backend/app/api/backend_logs.py ----------


---------- START OF FILE: backend/app/api/hids_logs.py ----------

from fastapi import APIRouter, Depends, Query, HTTPException
from app.core.security import get_current_active_user
from pathlib import Path
import os
import re

router = APIRouter(
    tags=["hids-logs"],
    dependencies=[Depends(get_current_active_user)]
)

LOG_DIR = Path(os.getenv("HIDS_LOG_DIR", "logs")).resolve()

# Regex pour parser les logs HIDS
HIDS_LOG_REGEX = re.compile(r"^(\d{4}-\d{2}-\d{2})\s(\d{2}:\d{2}:\d{2}),(\d{3})\s\|\s([A-Z]+)\s\|\s(.+?)\s\|\s(.+)$")
HIDS_ALERT_REGEX = re.compile(r"^(\d{4}-\d{2}-\d{2})\s(\d{2}:\d{2}:\d{2}),(\d{3})\s\|\s([A-Z]+)\s\|\s(.+)$")

def parse_hids_log(line: str):
    """Parse une ligne de log HIDS et retourne un dictionnaire."""
    line = line.strip()
    if not line:
        return None
    
    # Tente d'abord de faire correspondre la regex standard (avec la source)
    match = HIDS_LOG_REGEX.match(line)
    if match:
        date, time, ms, level, source, message = match.groups()
        return {
            "ts": f"{date}T{time}.{ms}Z",
            "level": level,
            "source": source.strip(),
            "msg": message.strip(),
            "text": line
        }

    # Tente de faire correspondre la regex alternative (sans la source)
    match = HIDS_ALERT_REGEX.match(line)
    if match:
        date, time, ms, level, message = match.groups()
        return {
            "ts": f"{date}T{time}.{ms}Z",
            "level": level,
            "source": "",
            "msg": message.strip(),
            "text": line
        }
    
    # Si aucune regex ne correspond, retourne la ligne brute
    return {
        "ts": None,
        "level": "RAW",
        "source": "",
        "msg": line,
        "text": line
    }

def read_log_file(filename: str):
    """Lit un fichier de log et renvoie les lignes parsées."""
    fp = (LOG_DIR / filename).resolve()
    
    if not str(fp).startswith(str(LOG_DIR)):
        raise HTTPException(status_code=400, detail="Invalid path")

    if not fp.exists():
        return []

    try:
        lines = fp.read_text(encoding="utf-8", errors="ignore").splitlines()
        parsed_lines = [parse_hids_log(line) for line in lines]
        return [l for l in parsed_lines if l is not None]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading file: {e}")

@router.get("/hids")
def get_hids_logs(
    log_type: str = Query("activity", description="activity|alerts"),
    page: int = Query(1, ge=1),
    limit: int = Query(15, ge=1, le=500),
    level: str | None = Query(None, description="DEBUG|INFO|WARNING|ERROR|CRITICAL"),
    contains: str | None = Query(None),
):
    """
    Endpoint pour récupérer les logs d'activité ou d'alerte.
    """
    if log_type == "alerts":
        filename = "alerts.log"
    else:
        filename = "hids.log"
    
    all_logs = read_log_file(filename)
    
    # Filtrer les logs
    filtered_logs = [
        log for log in all_logs
        if (not level or log['level'].lower() == level.lower()) and
           (not contains or contains.lower() in log['msg'].lower())
    ]
    
    # Inverser l'ordre pour les plus récents en premier
    filtered_logs.reverse()

    # Paginer les résultats
    total = len(filtered_logs)
    page_count = max(1, (total + limit - 1) // limit)
    page = min(page, page_count)
    start_index = (page - 1) * limit
    end_index = start_index + limit
    paginated_logs = filtered_logs[start_index:end_index]
    
    return {
        "lines": paginated_logs,
        "total": total,
        "page_count": page_count
    }

---------- END OF FILE: backend/app/api/hids_logs.py ----------


---------- START OF FILE: backend/app/api/logs.py ----------

# backend/app/api/logs.py
import os
import re
from pathlib import Path
from datetime import datetime
from typing import Optional

from fastapi import APIRouter, Depends, Query, HTTPException
from pydantic import BaseModel, Field

from app.core.security import get_current_active_user

# ⚠️ Définir le router AVANT d'utiliser les décorateurs
router = APIRouter(
    prefix="/logs",
    tags=["logs"],
    dependencies=[Depends(get_current_active_user)]
)

LOG_DIR = Path(os.getenv("HIDS_LOG_DIR", "logs")).resolve()

# Regex pour les logs d'activité et d'alerte avec et sans la source
HIDS_LOG_REGEX_FULL = re.compile(
    r"^(\d{4}-\d{2}-\d{2})\s(\d{2}:\d{2}:\d{2}),(\d{3})\s\|\s([A-Z]+)\s\|\s(.+?)\s\|\s(.+)$"
)
HIDS_LOG_REGEX_NO_SOURCE = re.compile(
    r"^(\d{4}-\d{2}-\d{2})\s(\d{2}:\d{2}:\d{2}),(\d{3})\s\|\s([A-Z]+)\s\|\s(.+)$"
)

def parse_log_line(line: str):
    """Parse une ligne de log HIDS et retourne un dictionnaire."""
    line = line.strip()
    if not line:
        return None

    match = HIDS_LOG_REGEX_FULL.match(line)
    if match:
        date, time, ms, level, source, message = match.groups()
        return {
            "ts": f"{date}T{time}.{ms}Z",
            "level": level,
            "source": source.strip(),
            "msg": message.strip(),
            "text": line
        }

    match = HIDS_LOG_REGEX_NO_SOURCE.match(line)
    if match:
        date, time, ms, level, message = match.groups()
        return {
            "ts": f"{date}T{time}.{ms}Z",
            "level": level,
            "source": "",
            "msg": message.strip(),
            "text": line
        }

    return {
        "ts": None,
        "level": "RAW",
        "source": "",
        "msg": line,
        "text": line
    }

def read_log_file(filename: str):
    """Lit un fichier de log et renvoie les lignes parsées."""
    fp = (LOG_DIR / filename).resolve()

    if not str(fp).startswith(str(LOG_DIR)):
        raise HTTPException(status_code=400, detail="Invalid path")

    if not fp.exists():
        return []

    try:
        lines = fp.read_text(encoding="utf-8", errors="ignore").splitlines()
        parsed_lines = [parse_log_line(line) for line in lines]
        return [l for l in parsed_lines if l is not None]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reading file: {e}")

@router.get("/hids")
def get_hids_logs(
    log_type: str = Query("activity", description="Type de log: 'activity' ou 'alerts'"),
    page: int = Query(1, ge=1),
    limit: int = Query(15, ge=1, le=500),
    level: Optional[str] = Query(None, description="DEBUG|INFO|WARNING|ERROR|CRITICAL"),
    contains: Optional[str] = Query(None),
):
    """
    Endpoint unifié pour récupérer les logs d'activité ou d'alerte.
    """
    if log_type not in ["activity", "alerts"]:
        raise HTTPException(status_code=400, detail="Log type must be 'activity' or 'alerts'")

    filename = "hids.log" if log_type == "activity" else "alerts.log"
    all_logs = read_log_file(filename)

    filtered_logs = [
        log for log in all_logs
        if (not level or (log.get('level') or '').lower() == level.lower()) and
            (not contains or contains.lower() in (log.get('msg') or '').lower())
    ]

    filtered_logs.reverse()

    total = len(filtered_logs)
    page_count = max(1, (total + limit - 1) // limit)
    page = min(page, page_count)
    start_index = (page - 1) * limit
    end_index = start_index + limit
    paginated_logs = filtered_logs[start_index:end_index]

    return {
        "lines": paginated_logs,
        "total": total,
        "page_count": page_count
    }

# ---------- Clear & Purge (compat front) ----------

class ClearBody(BaseModel):
    type: str = Field("activity", description="'activity' | 'alerts'")

class PurgeBody(BaseModel):
    type: str                         # 'activity' | 'alerts'
    level: Optional[str] = None
    from_: Optional[str] = Field(None, alias="from")  # accepte 'from' depuis le front
    to: Optional[str] = None

def _parse_iso(s: Optional[str]) -> Optional[datetime]:
    if not s:
        return None
    try:
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        return None

def _ensure_admin(current_user=Depends(get_current_active_user)):
    if not getattr(current_user, "is_admin", False):
        raise HTTPException(status_code=403, detail="Admin privileges required")
    return current_user

@router.post("/hids/clear", dependencies=[Depends(_ensure_admin)])
def clear_hids_logs(body: ClearBody):
    """
    Vide entièrement le fichier de logs demandé.
    Corps attendu (front): { "type": "activity" | "alerts" }
    """
    log_type = body.type
    if log_type not in ["activity", "alerts"]:
        raise HTTPException(status_code=400, detail="Log type must be 'activity' or 'alerts'")

    filename = "hids.log" if log_type == "activity" else "alerts.log"
    fp = (LOG_DIR / filename).resolve()

    if not str(fp).startswith(str(LOG_DIR)):
        raise HTTPException(status_code=400, detail="Invalid path")

    try:
        LOG_DIR.mkdir(parents=True, exist_ok=True)
        if fp.exists():
            fp.write_text("", encoding="utf-8")
        return {"status": "success", "message": f"Log file '{filename}' cleared."}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error clearing file: {e}")

@router.post("/hids/purge", dependencies=[Depends(_ensure_admin)])
def purge_hids_logs(body: PurgeBody):
    """
    Supprime les lignes correspondant aux filtres.
    Corps attendu (front): { type, level?, from?, to? }
    """
    if body.type not in ["activity", "alerts"]:
        raise HTTPException(status_code=400, detail="type must be 'activity' or 'alerts'")

    filename = "hids.log" if body.type == "activity" else "alerts.log"
    fp = (LOG_DIR / filename).resolve()

    if not str(fp).startswith(str(LOG_DIR)):
        raise HTTPException(status_code=400, detail="Invalid path")

    if not fp.exists():
        return {"status": "success", "message": "Nothing to purge."}

    lines = fp.read_text(encoding="utf-8", errors="ignore").splitlines()
    parsed = [parse_log_line(l) for l in lines]

    t_from = _parse_iso(body.from_)
    t_to = _parse_iso(body.to)
    level = (body.level or "").upper() if body.level else None

    def should_be_purged(log):
        """Retourne True si la ligne doit être supprimée."""
        if not log:
            return False

        # Si aucun filtre n'est actif, on ne supprime rien
        if not level and not t_from and not t_to:
            return False

        # Vérifier le niveau
        if level and (log.get("level") or "").upper() != level:
            return False # Le niveau ne correspond pas, on ne supprime pas

        # Vérifier la période
        ts = log.get("ts")
        t = _parse_iso(ts) if ts else None
        if t_from and (not t or t < t_from):
            return False # En dehors de la période, on ne supprime pas
        if t_to and (not t or t > t_to):
            return False # En dehors de la période, on ne supprime pas

        return True # Tous les filtres actifs correspondent, on supprime

    kept_text = [p["text"] if p else lines[i] for i, p in enumerate(parsed) if not should_be_purged(p)]
    fp.write_text("\n".join(kept_text) + ("\n" if kept_text else ""), encoding="utf-8")
    purged = len(lines) - len(kept_text)
    return {"status": "success", "purged": purged}

---------- END OF FILE: backend/app/api/logs.py ----------


---------- START OF FILE: backend/app/models/alerts.py ----------

# backend/app/models/alerts.py
from pydantic import BaseModel
from typing import Any, Optional, List
from datetime import datetime

class AlertOut(BaseModel):
    id: int | None = None
    ts: datetime | None = None
    severity: str | None = None
    rule: Optional[str] = None
    entity_type: Optional[str] = None
    entity_id: Optional[int] = None
    entity_label: Optional[str] = None
    message: Optional[str] = None
    meta: Optional[Any] = None
    class Config:
        from_attributes = True

class ActivityOut(BaseModel):
    id: int | None = None
    ts: datetime | None = None
    kind: str | None = None
    entity_type: Optional[str] = None
    entity_id: Optional[int] = None
    entity_label: Optional[str] = None
    message: Optional[str] = None
    meta: Optional[Any] = None
    class Config:
        from_attributes = True

class PageOut(BaseModel):
    items: List[Any]
    count: int
    total: int

---------- END OF FILE: backend/app/models/alerts.py ----------


---------- START OF FILE: backend/app/services/alert_service.py ----------


---------- END OF FILE: backend/app/services/alert_service.py ----------


---------- START OF FILE: backend/app/services/report_service.py ----------


---------- END OF FILE: backend/app/services/report_service.py ----------


---------- START OF FILE: backend/app/services/scan_tasks.py ----------

import os, json, logging, hashlib
from datetime import datetime
from sqlalchemy.orm import Session

from app.db.session import SessionLocal
from app.db.models import MonitoredFile, MonitoredFolder
from .hash_service import calculate_file_hash, calculate_folder_fingerprint, check_ip_activity

# Configuration logging
LOG_PATH = os.getenv("HIDS_LOG_PATH", "logs/hids.log")
os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)

logger = logging.getLogger("hids-scanner")
logger.setLevel(logging.INFO)
if not logger.handlers:
    fh = logging.FileHandler(LOG_PATH)
    fmt = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    fh.setFormatter(fmt)
    logger.addHandler(fh)

def _write_alert(event_type: str, item_id: int, details: dict):
    """Écrit une alerte dans les logs"""
    event = {
        "type": "alert",
        "alert_type": event_type,
        "item_id": item_id,
        "timestamp": datetime.utcnow().isoformat(),
        **details
    }
    logger.warning(json.dumps(event, ensure_ascii=False))

def _write_activity(event_type: str, item_id: int, details: dict):
    """Écrit une activité normale dans les logs"""
    event = {
        "type": "activity",
        "activity_type": event_type,
        "item_id": item_id,
        "timestamp": datetime.utcnow().isoformat(),
        **details
    }
    logger.info(json.dumps(event, ensure_ascii=False))

def scan_file(item_id: int, path: str):
    """Scan réel d'un fichier avec comparaison de hash"""
    db: Session = SessionLocal()
    
    try:
        # Récupérer l'item depuis la base
        item = db.query(MonitoredFile).filter(MonitoredFile.id == item_id).first()
        if not item:
            _write_alert("file_error", item_id, {"error": "Item not found in database", "path": path})
            return

        # Vérifier si le fichier existe
        if not os.path.exists(path):
            _write_alert("file_not_found", item_id, {"path": path})
            # Mettre à jour le statut
            item.current_hash = None
            item.last_scan = datetime.utcnow()
            db.commit()
            return

        # Calculer le hash actuel
        current_hash = calculate_file_hash(path)
        if not current_hash:
            _write_alert("file_hash_error", item_id, {"path": path})
            return

        # Premier scan : établir la baseline
        if not item.baseline_hash:
            item.baseline_hash = current_hash
            item.current_hash = current_hash
            item.last_scan = datetime.utcnow()
            db.commit()
            _write_activity("file_baseline_established", item_id, {
                "path": path, 
                "baseline_hash": current_hash
            })
            return

        # Scans suivants : comparer avec la baseline
        item.current_hash = current_hash
        item.last_scan = datetime.utcnow()
        db.commit()

        if current_hash != item.baseline_hash:
            _write_alert("file_modified", item_id, {
                "path": path,
                "previous_hash": item.baseline_hash,
                "current_hash": current_hash
            })
        else:
            _write_activity("file_unchanged", item_id, {"path": path})

    except Exception as e:
        _write_alert("file_scan_error", item_id, {"path": path, "error": str(e)})
    finally:
        db.close()

def scan_folder(item_id: int, path: str):
    """Scan réel d'un dossier"""
    db: Session = SessionLocal()
    
    try:
        item = db.query(MonitoredFolder).filter(MonitoredFolder.id == item_id).first()
        if not item:
            _write_alert("folder_error", item_id, {"error": "Item not found in database", "path": path})
            return

        if not os.path.exists(path) or not os.path.isdir(path):
            _write_alert("folder_not_found", item_id, {"path": path})
            item.folder_hash = None
            item.file_count = 0
            item.last_scan = datetime.utcnow()
            db.commit()
            return

        # Calculer l'empreinte du dossier
        fingerprint = calculate_folder_fingerprint(path)
        if not fingerprint:
            _write_alert("folder_scan_error", item_id, {"path": path})
            return

        current_hash, file_count = fingerprint

        # Premier scan
        if not item.folder_hash:
            item.folder_hash = current_hash
            item.file_count = file_count
            item.last_scan = datetime.utcnow()
            db.commit()
            _write_activity("folder_baseline_established", item_id, {
                "path": path,
                "baseline_hash": current_hash,
                "file_count": file_count
            })
            return

        # Scans suivants
        previous_file_count = item.file_count
        item.file_count = file_count
        item.last_scan = datetime.utcnow()
        db.commit()

        if current_hash != item.folder_hash:
            _write_alert("folder_modified", item_id, {
                "path": path,
                "previous_hash": item.folder_hash,
                "current_hash": current_hash,
                "previous_file_count": previous_file_count,
                "current_file_count": file_count
            })
            
            # Mettre à jour la baseline pour les prochains scans
            item.folder_hash = current_hash
            db.commit()
        else:
            _write_activity("folder_unchanged", item_id, {
                "path": path, 
                "file_count": file_count
            })

    except Exception as e:
        _write_alert("folder_scan_error", item_id, {"path": path, "error": str(e)})
    finally:
        db.close()

def scan_ip(item_id: int, ip: str, hostname: str = None):
    """Scan réel d'une IP"""
    db: Session = SessionLocal()
    
    try:
        item = db.query(MonitoredIP).filter(MonitoredIP.id == item_id).first()
        if not item:
            _write_alert("ip_error", item_id, {"error": "Item not found in database", "ip": ip})
            return

        # Vérifier l'activité de l'IP
        ip_status = check_ip_activity(ip)
        
        # Premier scan
        if not hasattr(item, 'last_status') or not item.last_status:
            item.last_status = json.dumps(ip_status)
            item.last_scan = datetime.utcnow()
            db.commit()
            _write_activity("ip_baseline_established", item_id, {
                "ip": ip,
                "status": ip_status
            })
            return

        # Comparaison avec le statut précédent
        previous_status = json.loads(item.last_status)
        item.last_status = json.dumps(ip_status)
        item.last_scan = datetime.utcnow()
        db.commit()

        # Détection des changements
        if previous_status.get('is_active') != ip_status.get('is_active'):
            _write_alert("ip_status_changed", item_id, {
                "ip": ip,
                "previous_active": previous_status.get('is_active'),
                "current_active": ip_status.get('is_active'),
                "connections": ip_status.get('connections', [])
            })

    except Exception as e:
        _write_alert("ip_scan_error", item_id, {"ip": ip, "error": str(e)})
    finally:
        db.close()
---------- END OF FILE: backend/app/services/scan_tasks.py ----------


---------- START OF FILE: backend/app/main.py ----------

from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
from app.core.logging import logger
from app.core.config import settings
from app.db.base import Base
from app.db.session import engine as db_engine
Base.metadata.create_all(bind=db_engine)

from app.core.scheduler import start_scheduler, shutdown_scheduler, add_interval_job, FREQ_SECONDS
from app.services.scan_tasks import scan_file, scan_folder, scan_ip
from app.db.session import SessionLocal
from app.db.models import MonitoredFile, MonitoredFolder, MonitoredIP

from app.api.status import router as status_router
from app.api.auth import router as auth_router
from app.api.users import router as users_router
from app.api.monitoring import router as monitoring_router
from app.api.metrics import router as metrics_router
from app.api.reports import router as reports_router
from app.api import engine as engine_routes
from app.api import fs 
# Importation du nouveau routeur unifié pour les logs
from app.api import logs

logger.info(f"Starting {settings.APP_NAME}... (version: {settings.VERSION})")


app = FastAPI(
    title=settings.APP_NAME,
    version=settings.VERSION,
    docs_url="/docs",
    redoc_url="/redoc"
)

# --- CORS ---
origins = [o.strip() for o in settings.ALLOWED_ORIGINS.split(",") if o.strip()]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# @app.on_event("startup")
# def on_startup():
#     """
#     Fonction de démarrage de l'application.
#     """
#     logger.info("Creating database tables (if not exist)...")
    
#     Base.metadata.create_all(bind=db_engine)
#     logger.info("Database ready.")
    
#     start_scheduler()
#     db = SessionLocal()
#     try:
#         for f in db.query(MonitoredFile).filter(MonitoredFile.status == "active").all():
#             sec = FREQ_SECONDS.get(getattr(f, "frequency", "hourly"), 3600)
#             add_interval_job("file", f.id, sec, scan_file, item_id=f.id, path=f.path)
#         for d in db.query(MonitoredFolder).filter(MonitoredFolder.status == "active").all():
#             sec = FREQ_SECONDS.get(getattr(d, "frequency", "hourly"), 3600)
#             add_interval_job("folder", d.id, sec, scan_folder, item_id=d.id, path=d.path)
#         for i in db.query(MonitoredIP).filter(MonitoredIP.status == "active").all():
#             sec = FREQ_SECONDS.get(getattr(i, "frequency", "hourly"), 3600)
#             add_interval_job("ip", i.id, sec, scan_ip, item_id=i.id, ip=i.ip, hostname=i.hostname)
#     finally:
#         db.close()

@app.on_event("startup")
def on_startup():
    """
    Fonction de démarrage de l'application.
    """
    logger.info("Creating database tables (if not exist)...")
    Base.metadata.create_all(bind=db_engine)
    logger.info("Database ready.")
    
    # Démarrer le scheduler d'abord
    start_scheduler()
    logger.info("Scheduler started.")
    
    # Ensuite planifier les jobs
    db = SessionLocal()
    try:
        # Vérifier que le scheduler est actif
        from app.core.scheduler import get_scheduler
        scheduler = get_scheduler()
        if not scheduler.running:
            logger.error("Scheduler is not running!")
            return
            
        logger.info("Scheduling existing monitoring jobs...")
        
        for f in db.query(MonitoredFile).filter(MonitoredFile.status == "active").all():
            sec = FREQ_SECONDS.get(getattr(f, "frequency", "hourly"), 3600)
            add_interval_job("file", f.id, sec, scan_file, item_id=f.id, path=f.path)
            logger.info(f"Scheduled file monitoring: {f.path} (every {sec}s)")
            
        for d in db.query(MonitoredFolder).filter(MonitoredFolder.status == "active").all():
            sec = FREQ_SECONDS.get(getattr(d, "frequency", "hourly"), 3600)
            add_interval_job("folder", d.id, sec, scan_folder, item_id=d.id, path=d.path)
            logger.info(f"Scheduled folder monitoring: {d.path} (every {sec}s)")
            
        for i in db.query(MonitoredIP).filter(MonitoredIP.status == "active").all():
            sec = FREQ_SECONDS.get(getattr(i, "frequency", "hourly"), 3600)
            add_interval_job("ip", i.id, sec, scan_ip, item_id=i.id, ip=i.ip, hostname=i.hostname)
            logger.info(f"Scheduled IP monitoring: {i.ip} (every {sec}s)")
            
        logger.info("All monitoring jobs scheduled successfully.")
        
    except Exception as e:
        logger.error(f"Error during startup job scheduling: {e}")
    finally:
        db.close()


@app.on_event("shutdown")
def on_shutdown():
    """
    Fonction d'arrêt de l'application.
    """
    logger.info("Shutting down application...")
    
    shutdown_scheduler()
    
app.include_router(status_router, prefix="/api")
app.include_router(auth_router, prefix="/api")
app.include_router(users_router)
app.include_router(monitoring_router)
# Inclus le nouveau routeur unifié pour les logs
app.include_router(logs.router, prefix="/api")
app.include_router(metrics_router)
app.include_router(reports_router)
app.include_router(engine_routes.router)
app.include_router(fs.router)
---------- END OF FILE: backend/app/main.py ----------


---------- START OF FILE: hids-web/src/pages/AlertsLogs.jsx ----------

// src/pages/AlertsLogs.jsx

import { useEffect, useState } from "react";
import { api } from "../lib/api";
import { useAuth } from "../context/AuthProvider";
import { useNavigate } from "react-router-dom";

const PAGE_SIZE = 15;

// ── UI bits ────────────────────────────────────────────────────────────────────
function Badge({ children, className = "" }) {
  return (
    <span className={`px-2 py-1 rounded-full text-xs font-bold inline-flex items-center ${className}`}>
      {children}
    </span>
  );
}

// Composant LevelBadge mis à jour pour correspondre aux couleurs douces
const LevelBadge = ({ level }) => {
  const L = String(level || "").toUpperCase();
  const cls =
    L === "CRITICAL" ? "bg-red-500/30 text-white" :
    L === "HIGH"     ? "bg-red-500/20 text-red-300" :
    L === "MEDIUM"   ? "bg-yellow-500/20 text-yellow-300" :
    L === "LOW"      ? "bg-sky-500/20 text-sky-300" :
    L === "WARNING"  ? "bg-yellow-500/20 text-yellow-300" :
    L === "INFO"     ? "bg-green-500/20 text-green-300" :
                    "bg-white/5 text-muted";
  return <Badge className={cls}>{L || "—"}</Badge>;
};

function TableShell({ headers, children }) {
  return (
    <div className="card p-0 overflow-hidden">
      <table className="w-full text-sm">
        <thead className="bg-white/5 text-muted sticky top-0">
          <tr>
            {headers.map((h, i) => (
              <th key={i} className={`px-4 py-3 ${i === headers.length - 1 ? "text-right" : "text-left"}`}>{h}</th>
            ))}
          </tr>
        </thead>
        <tbody>{children}</tbody>
      </table>
    </div>
  );
}

function Pager({ page, pageCount, onPage }) {
  return (
    <div className="flex items-center justify-between gap-2">
      <div className="text-xs text-muted">Page {Math.min(page, pageCount) || 1} / {pageCount || 1}</div>
      <div className="flex gap-2">
        <button
          className="px-3 py-1.5 rounded-md border border-white/10 bg-panel/50 disabled:opacity-50"
          onClick={() => onPage(Math.max(1, page - 1))}
          disabled={page <= 1}
        >← Prev</button>
        <button
          className="px-3 py-1.5 rounded-md border border-white/10 bg-panel/50 disabled:opacity-50"
          onClick={() => onPage(Math.min(pageCount || 1, page + 1))}
          disabled={page >= (pageCount || 1)}
        >Next →</button>
      </div>
    </div>
  );
}

// ── Page ──────────────────────────────────────────────────────────────────────
export default function AlertsLogs() {
  const { token, user } = useAuth(); // on récupère l'objet user
  const navigate = useNavigate();

  const [tab, setTab] = useState("alerts"); // "alerts" | "activity"
  const [page, setPage] = useState(1);

  // filtres
  const [level, setLevel] = useState("");
  const [contains, setContains] = useState("");

  // données
  const [items, setItems] = useState([]);
  const [pageCount, setPageCount] = useState(1);
  const [loading, setLoading] = useState(false);

  // reset pagination si on change d’onglet
  useEffect(() => {
    setPage(1);
    setLevel("");
    setContains("");
  }, [tab]);

  // charge une page depuis le backend
  useEffect(() => {
    // Si le token n'est pas encore disponible, on ne fait pas l'appel
    if (!token) {
        return;
    }

    let stopped = false;
    (async () => {
      setLoading(true);
      try {
        const r = await api.listHidsLog(token, {
          type: tab, page, limit: PAGE_SIZE, level, contains
        });
        if (stopped) return;
        setItems(r.lines || []);
        setPageCount(r.page_count || 1);
      } catch (e) {
        console.error("Failed to fetch logs:", e);
        if (!stopped) {
          setItems([]);
          setPageCount(1);
        }
      } finally {
        if (!stopped) setLoading(false);
      }
    })();
    return () => { stopped = true; };
  }, [token, tab, page, level, contains]);

  const onClear = async () => {
    // On utilise une boîte de dialogue personnalisée au lieu de window.confirm
    // const confirmed = await showCustomConfirmDialog("⚠️ This will permanently clear logs. Continue?");
    if (!window.confirm(`⚠️ This will permanently clear ${tab} logs. Continue?`)) return;
    try {
      await api.clearHidsLog(token, tab);
      setPage(1);
    } catch (e) {
      // On utilise une boîte de dialogue personnalisée au lieu de window.alert
      // showCustomAlertDialog("Clear failed: " + (e?.body?.detail || e.message));
      window.alert("Clear failed: " + (e?.body?.detail || e.message));
    }
  };

  const headers = ["Date", "Time", "Level", "Source", "Message"];

  // Déterminer les niveaux de filtre en fonction de l'onglet actif
  const levels = tab === "alerts"
    ? ["CRITICAL", "HIGH", "MEDIUM", "LOW", "INFO"]
    : ["INFO", "WARNING", "ERROR"];

  const isAdmin = user?.is_admin; // Vérifie si l'utilisateur est admin

  return (
    <div className="space-y-6">
      {/* Tabs + Clear */}
      <div className="flex items-center justify-between">
        <div className="flex gap-2">
          {["alerts", "activity"].map(t => (
            <button
              key={t}
              onClick={() => setTab(t)}
              className={`px-3 py-1.5 rounded-md border capitalize ${tab===t ? "bg-panel2 border-white/10" : "bg-panel/50 border-white/5"}`}
            >{t}</button>
          ))}
        </div>

        <button
          onClick={onClear}
          disabled={!isAdmin} // Bouton désactivé si l'utilisateur n'est pas admin
          className="px-3 py-1.5 rounded-md border border-white/10 text-white disabled:opacity-50 disabled:cursor-not-allowed"
          title={`Clear ${tab} logs (admin only server-side)`}
          style={{
            background: isAdmin ? "linear-gradient(90deg, #d32f2f, #ef5350)" : "rgba(255, 255, 255, 0.05)",
            borderColor: isAdmin ? "#ef5350" : "rgba(255, 255, 255, 0.1)",
            color: isAdmin ? "white" : "rgba(255, 255, 255, 0.5)",
          }}
        >
          Clear logs
        </button>
      </div>

      {/* Filtres */}
      <div className="card p-3">
        <div className="flex flex-wrap gap-3">
          <select
            className="bg-panel2 border border-white/10 rounded-md text-sm px-2 py-1"
            value={level}
            onChange={e=>{ setLevel(e.target.value); setPage(1); }}
          >
            <option value="">Level: any</option>
            {levels.map(l => (
              <option key={l} value={l}>{l}</option>
            ))}
          </select>

          <input
            className="bg-panel2 border border-white/10 rounded-md text-sm px-2 py-1"
            placeholder="Search message…"
            value={contains}
            onChange={e=>{ setContains(e.target.value); setPage(1); }}
          />
        </div>
      </div>

      {/* Tableau */}
      <TableShell headers={headers}>
        {loading && (<tr><td colSpan={headers.length} className="px-4 py-6 text-muted">Loading…</td></tr>)}
        {!loading && items.length === 0 && (
          <tr><td colSpan={headers.length} className="px-4 py-6 text-muted">No log lines</td></tr>
        )}
        {!loading && items.map((ln, i)=>(
          <tr key={`${ln.ts}-${i}`} className="border-t border-white/5">
            <td className="px-4 py-2">{ln.ts ? new Date(ln.ts).toLocaleDateString() : "—"}</td>
            <td className="px-4 py-2 text-muted">{ln.ts ? new Date(ln.ts).toLocaleTimeString() : "—"}</td>
            <td className="px-4 py-2"><LevelBadge level={ln.level} /></td>
            <td className="px-4 py-2">{ln.source || "—"}</td>
            <td className="px-4 py-2 text-right font-mono">{ln.msg || "—"}</td>
          </tr>
        ))}
      </TableShell>

      <Pager page={page} pageCount={pageCount} onPage={setPage}/>
    </div>
  );
}

---------- END OF FILE: hids-web/src/pages/AlertsLogs.jsx ----------


